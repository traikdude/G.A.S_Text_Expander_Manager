"""
ğŸ¥ Text Expander Data Quality Analyzer
======================================
Comprehensive data health check for your shortcuts spreadsheet! ğŸ“Š
Identifies missing fields, empty values, and data quality issues! ğŸ”

Spreadsheet: Shortcuts
ID: 17NaZQTbIm8LEiO2VoQoIn5HpqGEQKGAIUXN81SGnZJQ

Run in Google Colab for best results! ğŸš€
"""

# %% [markdown]
# # ğŸ¥ Text Expander Data Quality Analyzer
# This notebook performs a comprehensive health check on your shortcuts data! âœ¨
# 
# **Features:**
# - ğŸ“Š Overall statistics and metrics
# - âŒ Missing field detection
# - ğŸ” Empty value finder
# - ğŸ“ Content length analysis
# - ğŸ·ï¸ Tag coverage report
# - ğŸŒ Language distribution
# - ğŸ“ˆ Category balance check
# - ğŸ“‹ Actionable fix recommendations

# %% [markdown]
# ## Step 1: Setup & Authentication ğŸ”

# %%
# Install required packages! ğŸ“¦
!pip install gspread google-auth pandas numpy matplotlib seaborn -q

print("âœ… Packages installed successfully! ğŸ“¦")

# %%
# Import all the goodies! ğŸ
import gspread
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from google.colab import auth
from google.auth import default
from collections import Counter
import warnings
warnings.filterwarnings('ignore')

# Set beautiful plot style! ğŸ¨
plt.style.use('seaborn-v0_8-whitegrid')
sns.set_palette("husl")

print("âœ… Libraries imported! Ready to analyze! ğŸš€")

# %%
# Authenticate with Google! ğŸ”‘
auth.authenticate_user()
creds, _ = default()
gc = gspread.authorize(creds)

print("âœ… Authentication successful! ğŸ”")

# %% [markdown]
# ## Step 2: Connect & Load Data ğŸ“¥

# %%
# Spreadsheet configuration! ğŸ“‹
SPREADSHEET_ID = "17NaZQTbIm8LEiO2VoQoIn5HpqGEQKGAIUXN81SGnZJQ"
SHEET_NAME = "Shortcuts"

# Connect to spreadsheet! ğŸ”—
try:
    spreadsheet = gc.open_by_key(SPREADSHEET_ID)
    worksheet = spreadsheet.worksheet(SHEET_NAME)
    print(f"âœ… Connected to '{spreadsheet.title}'! ğŸ“Š")
    print(f"   Sheet: '{SHEET_NAME}'")
    print(f"   Size: {worksheet.row_count} rows Ã— {worksheet.col_count} columns")
except Exception as e:
    print(f"âŒ Connection failed: {e}")

# %%
# Load all data into DataFrame! ğŸ“Š
data = worksheet.get_all_records()
df = pd.DataFrame(data)

print(f"âœ… Loaded {len(df)} shortcuts successfully! ğŸ‰")
print(f"\nğŸ“‹ Columns found: {list(df.columns)}")

# %% [markdown]
# ## Step 3: ğŸ“Š Overall Statistics Dashboard

# %%
def generate_overview_stats(df):
    """Generate comprehensive overview statistics! ğŸ“ˆ"""
    
    print("=" * 60)
    print("ğŸ“Š DATA QUALITY DASHBOARD")
    print("=" * 60)
    
    stats = {
        'ğŸ“‹ Total Shortcuts': len(df),
        'ğŸ“ Columns Available': len(df.columns),
    }
    
    # Core field analysis! ğŸ”
    core_fields = ['Snippet Name', 'Content', 'Application', 'Description', 'Language', 'Tags']
    
    print("\nğŸ” CORE FIELD ANALYSIS:")
    print("-" * 40)
    
    for field in core_fields:
        if field in df.columns:
            empty_count = df[field].isna().sum() + (df[field] == '').sum()
            filled_count = len(df) - empty_count
            fill_rate = (filled_count / len(df)) * 100
            
            # Emoji based on fill rate! ğŸ¨
            if fill_rate >= 90:
                status = "âœ…"
            elif fill_rate >= 70:
                status = "ğŸŸ¡"
            elif fill_rate >= 50:
                status = "ğŸŸ "
            else:
                status = "âŒ"
            
            print(f"  {status} {field}: {filled_count}/{len(df)} ({fill_rate:.1f}% filled)")
    
    # Enhanced fields (if available)! âœ¨
    enhanced_fields = ['MainCategory', 'Subcategory', 'FontStyle', 'Platform', 'UsageFrequency']
    available_enhanced = [f for f in enhanced_fields if f in df.columns]
    
    if available_enhanced:
        print("\nâœ¨ ENHANCED FIELD ANALYSIS:")
        print("-" * 40)
        
        for field in available_enhanced:
            empty_count = df[field].isna().sum() + (df[field] == '').sum()
            filled_count = len(df) - empty_count
            fill_rate = (filled_count / len(df)) * 100
            
            if fill_rate >= 90:
                status = "âœ…"
            elif fill_rate >= 70:
                status = "ğŸŸ¡"
            else:
                status = "âŒ"
            
            print(f"  {status} {field}: {filled_count}/{len(df)} ({fill_rate:.1f}% filled)")
    else:
        print("\nâš ï¸ Enhanced fields not yet added (run DropdownEnhancements first)!")
    
    return stats

overview = generate_overview_stats(df)

# %% [markdown]
# ## Step 4: âŒ Missing Field Report

# %%
def analyze_missing_fields(df):
    """Find all rows with missing critical data! ğŸ”"""
    
    print("\n" + "=" * 60)
    print("âŒ MISSING FIELD REPORT")
    print("=" * 60)
    
    issues = []
    
    # Check each critical field! ğŸ”
    critical_fields = {
        'Snippet Name': 'No snippet name - cannot identify!',
        'Content': 'No content - shortcut is useless!',
        'Description': 'No description - harder to categorize',
        'Language': 'No language - filtering limited',
        'Tags': 'No tags - search limited',
    }
    
    for field, impact in critical_fields.items():
        if field in df.columns:
            missing = df[(df[field].isna()) | (df[field] == '')]
            if len(missing) > 0:
                issues.append({
                    'field': field,
                    'count': len(missing),
                    'impact': impact,
                    'rows': missing.index.tolist()[:10]  # First 10 rows
                })
                print(f"\nâŒ {field}: {len(missing)} missing")
                print(f"   Impact: {impact}")
                if len(missing) <= 5:
                    for idx, row in missing.head().iterrows():
                        name = row.get('Snippet Name', f'Row {idx+2}')
                        print(f"   â†’ Row {idx+2}: {name[:50]}...")
    
    if not issues:
        print("\nâœ… No critical missing fields found! ğŸ‰")
    
    return issues

missing_report = analyze_missing_fields(df)

# %% [markdown]
# ## Step 5: ğŸ“ Content Length Analysis

# %%
def analyze_content_length(df):
    """Analyze content length distribution! ğŸ“"""
    
    print("\n" + "=" * 60)
    print("ğŸ“ CONTENT LENGTH ANALYSIS")
    print("=" * 60)
    
    if 'Content' not in df.columns:
        print("âŒ Content column not found!")
        return
    
    df['content_length'] = df['Content'].astype(str).str.len()
    
    print(f"\nğŸ“Š Length Statistics:")
    print(f"   ğŸ“ˆ Mean length: {df['content_length'].mean():.1f} characters")
    print(f"   ğŸ“‰ Min length: {df['content_length'].min()} characters")
    print(f"   ğŸ“ˆ Max length: {df['content_length'].max()} characters")
    print(f"   ğŸ“Š Median: {df['content_length'].median():.1f} characters")
    
    # Find extremes! âš ï¸
    very_short = df[df['content_length'] < 3]
    very_long = df[df['content_length'] > 1000]
    
    if len(very_short) > 0:
        print(f"\nâš ï¸ Very Short Snippets (<3 chars): {len(very_short)}")
        for idx, row in very_short.head(5).iterrows():
            print(f"   â†’ '{row['Snippet Name']}': '{row['Content']}'")
    
    if len(very_long) > 0:
        print(f"\nğŸ“¦ Long Snippets (>1000 chars): {len(very_long)}")
        for idx, row in very_long.head(5).iterrows():
            print(f"   â†’ '{row['Snippet Name']}': {row['content_length']} chars")
    
    # Create histogram! ğŸ“Š
    fig, ax = plt.subplots(figsize=(10, 5))
    
    # Filter out extreme outliers for visualization
    plot_data = df[df['content_length'] < 500]['content_length']
    
    ax.hist(plot_data, bins=50, color='#667eea', edgecolor='white', alpha=0.8)
    ax.set_xlabel('Content Length (characters)', fontsize=12)
    ax.set_ylabel('Number of Shortcuts', fontsize=12)
    ax.set_title('ğŸ“ Content Length Distribution', fontsize=14, fontweight='bold')
    ax.axvline(df['content_length'].mean(), color='red', linestyle='--', label=f'Mean: {df["content_length"].mean():.1f}')
    ax.legend()
    
    plt.tight_layout()
    plt.show()
    
    print("\nâœ… Histogram generated! ğŸ“ˆ")

analyze_content_length(df)

# %% [markdown]
# ## Step 6: ğŸŒ Language Distribution

# %%
def analyze_language_distribution(df):
    """Analyze language distribution! ğŸŒ"""
    
    print("\n" + "=" * 60)
    print("ğŸŒ LANGUAGE DISTRIBUTION")
    print("=" * 60)
    
    if 'Language' not in df.columns:
        print("âŒ Language column not found!")
        return
    
    # Clean and count languages! ğŸ“Š
    df['lang_clean'] = df['Language'].fillna('(empty)').replace('', '(empty)')
    lang_counts = df['lang_clean'].value_counts()
    
    print(f"\nğŸ“Š Languages Found: {len(lang_counts)}")
    print("-" * 40)
    
    for lang, count in lang_counts.items():
        pct = (count / len(df)) * 100
        bar = 'â–ˆ' * int(pct / 5) + 'â–’' * (20 - int(pct / 5))
        print(f"  {lang:20} {bar} {count:5} ({pct:5.1f}%)")
    
    # Pie chart! ğŸ¥§
    fig, ax = plt.subplots(figsize=(8, 8))
    colors = plt.cm.Set3(np.linspace(0, 1, len(lang_counts)))
    
    # Only show top 5 in pie, rest as "Other"
    top_langs = lang_counts.head(5)
    if len(lang_counts) > 5:
        other_count = lang_counts[5:].sum()
        top_langs['Other'] = other_count
    
    ax.pie(top_langs, labels=top_langs.index, autopct='%1.1f%%', colors=colors, startangle=90)
    ax.set_title('ğŸŒ Language Distribution', fontsize=14, fontweight='bold')
    
    plt.tight_layout()
    plt.show()

analyze_language_distribution(df)

# %% [markdown]
# ## Step 7: ğŸ·ï¸ Category Coverage (if available)

# %%
def analyze_category_coverage(df):
    """Analyze MainCategory coverage! ğŸ·ï¸"""
    
    print("\n" + "=" * 60)
    print("ğŸ·ï¸ CATEGORY COVERAGE ANALYSIS")
    print("=" * 60)
    
    if 'MainCategory' not in df.columns:
        print("âš ï¸ MainCategory column not found!")
        print("   Run the TextExpanderCategorizer notebook first! ğŸ")
        return
    
    # Clean and count! ğŸ“Š
    df['cat_clean'] = df['MainCategory'].fillna('(uncategorized)').replace('', '(uncategorized)')
    cat_counts = df['cat_clean'].value_counts()
    
    print(f"\nğŸ“Š Categories Found: {len(cat_counts)}")
    print("-" * 50)
    
    for cat, count in cat_counts.items():
        pct = (count / len(df)) * 100
        bar = 'â–ˆ' * int(pct / 2) + 'â–’' * (50 - int(pct / 2))
        emoji = cat[0] if cat and cat[0] in 'ğŸ¯ğŸ”£ğŸ˜ŠğŸ“…ğŸ”¢ğŸ’¬ğŸ“§ğŸ¨ğŸŒˆğŸ·ï¸' else 'â“'
        print(f"  {emoji} {cat[:35]:35} {count:5} ({pct:5.1f}%)")
    
    # Check for uncategorized! âš ï¸
    uncategorized = df[df['cat_clean'] == '(uncategorized)']
    if len(uncategorized) > 0:
        print(f"\nâš ï¸ Uncategorized Shortcuts: {len(uncategorized)}")
        print("   These need manual categorization or re-running the categorizer!")

analyze_category_coverage(df)

# %% [markdown]
# ## Step 8: ğŸ“‹ Data Quality Score

# %%
def calculate_quality_score(df):
    """Calculate overall data quality score! ğŸ†"""
    
    print("\n" + "=" * 60)
    print("ğŸ† OVERALL DATA QUALITY SCORE")
    print("=" * 60)
    
    scores = {}
    
    # Score each dimension (0-100)! ğŸ“Š
    
    # 1. Completeness - are all fields filled? ğŸ“‹
    core_fields = ['Snippet Name', 'Content', 'Description', 'Language', 'Tags']
    available_fields = [f for f in core_fields if f in df.columns]
    
    completeness_scores = []
    for field in available_fields:
        filled = len(df) - df[field].isna().sum() - (df[field] == '').sum()
        completeness_scores.append((filled / len(df)) * 100)
    
    scores['ğŸ“‹ Completeness'] = np.mean(completeness_scores) if completeness_scores else 0
    
    # 2. Uniqueness - no duplicates? ğŸ”„
    if 'Content' in df.columns:
        unique_ratio = df['Content'].nunique() / len(df) * 100
        scores['ğŸ”„ Uniqueness'] = unique_ratio
    
    # 3. Validity - content length reasonable? âœ…
    if 'Content' in df.columns:
        df['_len'] = df['Content'].astype(str).str.len()
        valid = len(df[(df['_len'] >= 1) & (df['_len'] <= 10000)])
        scores['âœ… Validity'] = (valid / len(df)) * 100
    
    # 4. Categorization - if available ğŸ·ï¸
    if 'MainCategory' in df.columns:
        categorized = len(df[(df['MainCategory'].notna()) & (df['MainCategory'] != '')])
        scores['ğŸ·ï¸ Categorized'] = (categorized / len(df)) * 100
    
    # Calculate overall! ğŸ¯
    overall = np.mean(list(scores.values()))
    
    print(f"\nğŸ“Š Dimension Scores:")
    print("-" * 40)
    
    for dimension, score in scores.items():
        bar = 'â–ˆ' * int(score / 5) + 'â–‘' * (20 - int(score / 5))
        grade = 'ğŸŸ¢' if score >= 80 else 'ğŸŸ¡' if score >= 60 else 'ğŸ”´'
        print(f"  {grade} {dimension}: {bar} {score:.1f}%")
    
    print(f"\n{'='*40}")
    
    # Overall grade with emoji! ğŸ†
    if overall >= 90:
        grade_emoji = "ğŸ†"
        grade_text = "EXCELLENT"
    elif overall >= 80:
        grade_emoji = "ğŸ¥‡"
        grade_text = "GREAT"
    elif overall >= 70:
        grade_emoji = "ğŸ¥ˆ"
        grade_text = "GOOD"
    elif overall >= 60:
        grade_emoji = "ğŸ¥‰"
        grade_text = "FAIR"
    else:
        grade_emoji = "ğŸ“ˆ"
        grade_text = "NEEDS WORK"
    
    print(f"  {grade_emoji} OVERALL SCORE: {overall:.1f}% - {grade_text}")
    print(f"{'='*40}")
    
    return overall, scores

quality_score, dimension_scores = calculate_quality_score(df)

# %% [markdown]
# ## Step 9: ğŸ“‹ Actionable Recommendations

# %%
def generate_recommendations(df):
    """Generate actionable fix recommendations! ğŸ’¡"""
    
    print("\n" + "=" * 60)
    print("ğŸ’¡ ACTIONABLE RECOMMENDATIONS")
    print("=" * 60)
    
    recommendations = []
    
    # Check each issue and recommend! ğŸ”§
    
    # 1. Missing descriptions? âŒ
    if 'Description' in df.columns:
        missing_desc = len(df[(df['Description'].isna()) | (df['Description'] == '')])
        if missing_desc > 0:
            recommendations.append({
                'priority': 'ğŸ”´ HIGH',
                'issue': f'{missing_desc} shortcuts missing descriptions',
                'action': 'Run auto-categorizer or manually add descriptions',
                'impact': 'Improves filtering and searchability'
            })
    
    # 2. Missing tags? ğŸ·ï¸
    if 'Tags' in df.columns:
        missing_tags = len(df[(df['Tags'].isna()) | (df['Tags'] == '')])
        if missing_tags > 0:
            pct = (missing_tags / len(df)) * 100
            priority = 'ğŸ”´ HIGH' if pct > 50 else 'ğŸŸ¡ MEDIUM' if pct > 20 else 'ğŸŸ¢ LOW'
            recommendations.append({
                'priority': priority,
                'issue': f'{missing_tags} shortcuts missing tags ({pct:.1f}%)',
                'action': 'Use Smart Tag Generator notebook',
                'impact': 'Improves search functionality'
            })
    
    # 3. Missing language? ğŸŒ
    if 'Language' in df.columns:
        missing_lang = len(df[(df['Language'].isna()) | (df['Language'] == '')])
        if missing_lang > 0:
            recommendations.append({
                'priority': 'ğŸŸ¡ MEDIUM',
                'issue': f'{missing_lang} shortcuts missing language',
                'action': 'Use Language Detection notebook',
                'impact': 'Enables language-based filtering'
            })
    
    # 4. Missing categories? ğŸ·ï¸
    if 'MainCategory' in df.columns:
        missing_cat = len(df[(df['MainCategory'].isna()) | (df['MainCategory'] == '')])
        if missing_cat > 0:
            recommendations.append({
                'priority': 'ğŸ”´ HIGH',
                'issue': f'{missing_cat} shortcuts uncategorized',
                'action': 'Re-run TextExpanderCategorizer with fixes',
                'impact': 'Essential for new filter UI'
            })
    elif 'MainCategory' not in df.columns:
        recommendations.append({
            'priority': 'ğŸ”´ HIGH',
            'issue': 'MainCategory column not yet added',
            'action': 'Run clasp push, then Add Enhanced Dropdowns in spreadsheet',
            'impact': 'Required for category-based filtering'
        })
    
    # Print recommendations! ğŸ“‹
    if recommendations:
        for i, rec in enumerate(recommendations, 1):
            print(f"\n{rec['priority']} Recommendation #{i}:")
            print(f"   Issue: {rec['issue']}")
            print(f"   Action: {rec['action']}")
            print(f"   Impact: {rec['impact']}")
    else:
        print("\nâœ… No critical issues found! Your data is in great shape! ğŸ‰")
    
    return recommendations

recommendations = generate_recommendations(df)

# %% [markdown]
# ## Step 10: ğŸ“Š Export Quality Report

# %%
def export_quality_report(df, quality_score):
    """Export comprehensive quality report! ğŸ“¤"""
    
    print("\n" + "=" * 60)
    print("ğŸ“¤ EXPORTING QUALITY REPORT")
    print("=" * 60)
    
    # Create summary DataFrame! ğŸ“Š
    report_data = []
    
    for idx, row in df.iterrows():
        issues = []
        
        # Check each field
        if pd.isna(row.get('Description')) or row.get('Description') == '':
            issues.append('Missing Description')
        if pd.isna(row.get('Tags')) or row.get('Tags') == '':
            issues.append('Missing Tags')
        if pd.isna(row.get('Language')) or row.get('Language') == '':
            issues.append('Missing Language')
        if 'MainCategory' in df.columns:
            if pd.isna(row.get('MainCategory')) or row.get('MainCategory') == '':
                issues.append('Missing Category')
        
        content_len = len(str(row.get('Content', '')))
        if content_len < 3:
            issues.append('Very Short Content')
        if content_len > 5000:
            issues.append('Very Long Content')
        
        report_data.append({
            'Row': idx + 2,
            'Snippet Name': row.get('Snippet Name', '')[:50],
            'Content Length': content_len,
            'Issues Count': len(issues),
            'Issues': ', '.join(issues) if issues else 'None'
        })
    
    report_df = pd.DataFrame(report_data)
    
    # Save to CSV! ğŸ’¾
    report_df.to_csv('/content/data_quality_report.csv', index=False)
    
    # Also save rows with issues only
    issues_df = report_df[report_df['Issues Count'] > 0]
    issues_df.to_csv('/content/rows_with_issues.csv', index=False)
    
    print(f"âœ… Full report saved: data_quality_report.csv")
    print(f"âœ… Issues report saved: rows_with_issues.csv ({len(issues_df)} rows)")
    
    # Download files! ğŸ“¥
    from google.colab import files
    files.download('/content/data_quality_report.csv')
    files.download('/content/rows_with_issues.csv')
    
    return report_df

report_df = export_quality_report(df, quality_score)

# %% [markdown]
# ## ğŸ‰ Analysis Complete!
# 
# Your data quality report has been generated! ğŸ“Š
# 
# **Next Steps:**
# 1. Review the downloaded CSV files ğŸ“‹
# 2. Fix high-priority issues first ğŸ”´
# 3. Re-run the categorizer if needed ğŸ·ï¸
# 4. Run this analyzer again to track improvement! ğŸ“ˆ
