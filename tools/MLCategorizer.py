"""
ðŸ§  Text Expander ML Categorizer
===============================
Machine Learning-powered categorization using scikit-learn! ðŸ¤–
Smarter than regex - learns from your existing categorizations! âœ¨

ðŸŒ Works in BOTH:
   - Google Colab (recommended for ML)
   - Local Python (python MLCategorizer.py)

Spreadsheet: Shortcuts
ID: 17NaZQTbIm8LEiO2VoQoIn5HpqGEQKGAIUXN81SGnZJQ
"""

# Configuration Constants
SPREADSHEET_ID = "17NaZQTbIm8LEiO2VoQoIn5HpqGEQKGAIUXN81SGnZJQ"
SHEET_NAME = "Shortcuts"

# ML Hyperparameters
TFIDF_MAX_FEATURES = 1000
TFIDF_NGRAM_RANGE = (1, 2)
TEST_SIZE = 0.2
RANDOM_STATE = 42
CV_FOLDS = 5
MIN_TRAINING_SAMPLES = 5

# %% [markdown]
# # ðŸ§  ML Categorizer
# Train a machine learning model on your categorized shortcuts!

# %% [markdown]
# ## Step 1: Setup ðŸ”

# %%
# %%
import sys
from pathlib import Path

# Add current directory to path so we can import colab_compat if run from adjacent dir
current_dir = Path(__file__).resolve().parent
if str(current_dir) not in sys.path:
    sys.path.insert(0, str(current_dir))

try:
    from colab_compat import ColabCompat
except ImportError:
    # Fallback if tools/ is not in path
    sys.path.append("tools")
    from tools.colab_compat import ColabCompat

# Initialize Compatibility Layer
compat = ColabCompat()
compat.print_environment()
compat.ensure_packages(["scikit-learn", "matplotlib", "seaborn"])

# %%
import gspread
import pandas as pd
import numpy as np
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.pipeline import Pipeline
import matplotlib.pyplot as plt
import seaborn as sns

print("âœ… Libraries imported!")

# %% [markdown]
# ## Step 2: Authentication ðŸ”

# %%
# %%
MOCK_MODE = False
try:
    gc = compat.get_gspread_client()
    print("âœ… Authenticated successfully!")
except Exception as e:
    print(f"âš ï¸ Authentication failed: {e}")
    print("ðŸš€ Switching to MOCK MODE for testing/demo purposes.")
    MOCK_MODE = True
    gc = None

# %% [markdown]
# ## Step 3: Load Data ðŸ“Š

# %%
OUTPUT_FOLDER = str(compat.base_path)

# Initialize dataframe
df = None
worksheet = None

if MOCK_MODE:
    print("\nðŸš§ MOCK MODE: Generating dummy data for logic testing...")
    # Create valid dummy dataframe matching expected structure
    data = {
        'Snippet Name': ['addr', 'mail', 'sig', 'meeting', 'date'],
        'Content': ['123 Main St', 'example@test.com', 'Cheers,\nErik', 'Meeting link:', '2024-01-01'],
        'Description': ['Home address', 'Personal email', 'Signature', 'Zoom link', 'Current date'],
        'MainCategory': ['Contact', 'Contact', 'Communication', 'Communication', 'Dates & Time'],
        'Subcategory': ['Address', 'Email', 'Signatures', 'Meetings', 'Date Formats']
    }
    df = pd.DataFrame(data)
    print(f"ðŸ“Š Mock Data Loaded: {len(df)} rows")
    
    # Simulate partial categorization for testing
    categorized_count = len(df)
    print(f"ðŸ“Š Categorized: {categorized_count} / {len(df)}")

else:
    try:
        sh = gc.open_by_key(SPREADSHEET_ID)
        worksheet = sh.worksheet(SHEET_NAME)
        # Using get_all_records is safer for headers
        df = pd.DataFrame(worksheet.get_all_records())
        print(f"âœ… Loaded {len(df)} shortcuts!")
        
        # Check for existing categories
        if 'MainCategory' in df.columns:
            # Handle empty strings or NaNs
            categorized = df['MainCategory'].replace('', np.nan).notna()
            print(f"ðŸ“Š Categorized: {categorized.sum()} / {len(df)}")
        else:
            print("âš ï¸ 'MainCategory' column missing. Adding empty column for compatibility.")
            df['MainCategory'] = ''
            
    except Exception as e:
        print(f"âŒ Error loading spreadsheet: {e}")
        raise

# Logic Hardening: Ensure 'MainCategory' exists (D13 Requirement)
if df is not None:
    if 'MainCategory' not in df.columns:
        df['MainCategory'] = ''
    # Fill NaNs with empty strings to prevent 'float' errors in text processing
    df['MainCategory'] = df['MainCategory'].fillna('')

print("âœ… Data validation complete.")

# %% [markdown]
# ## Step 4: Prepare Training Data ðŸ“š

# %%
def prepare_training_data(min_samples=MIN_TRAINING_SAMPLES):
    """Prepare data for ML training! ðŸ“š"""
    print("\n" + "=" * 60)
    print("ðŸ“š PREPARING TRAINING DATA")
    print("=" * 60)
    
    if df is None:
        print("âŒ No data loaded! Check spreadsheet connection.")
        return None, None, None
    
    if 'MainCategory' not in df.columns:
        print("âŒ No MainCategory column!")
        return None, None, None
    
    # Combine text features
    df['combined_text'] = df.apply(
        lambda row: f"{row.get('Content', '')} {row.get('Description', '')} {row.get('Snippet Name', '')}",
        axis=1
    )
    
    # Filter to categorized rows
    df_cat = df[df['MainCategory'].notna() & (df['MainCategory'] != '')].copy()
    
    print(f"ðŸ“Š Categorized rows: {len(df_cat)}")
    
    # Filter categories with enough samples
    cat_counts = df_cat['MainCategory'].value_counts()
    valid_cats = cat_counts[cat_counts >= min_samples].index.tolist()
    
    df_train = df_cat[df_cat['MainCategory'].isin(valid_cats)]
    df_predict = df[~df.index.isin(df_train.index) | (df['MainCategory'] == '')]
    
    print(f"âœ… Training samples: {len(df_train)}")
    print(f"ðŸŽ¯ To predict: {len(df_predict)}")
    print(f"ðŸ“‹ Valid categories: {len(valid_cats)}")
    
    return df, df_train, df_predict

df_all, df_train, df_predict = prepare_training_data()

# %% [markdown]
# ## Step 5: Train ML Model ðŸ§ 

# %%
model = None
valid_categories = None

def train_model():
    """Train the ML categorizer! ðŸ§ """
    global model, valid_categories
    
    print("\n" + "=" * 60)
    print("ðŸ§  TRAINING ML MODEL")
    print("=" * 60)
    
    if df_train is None or len(df_train) < 10:
        print("âŒ Not enough training data!")
        return None
    
    X = df_train['combined_text'].astype(str)
    y = df_train['MainCategory']
    
    valid_categories = y.unique().tolist()
    
    # Split data
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    
    print(f"ðŸ“Š Training set: {len(X_train)}")
    print(f"ðŸ“Š Test set: {len(X_test)}")
    
    # Create pipeline with configurable hyperparameters
    model = Pipeline([
        ('tfidf', TfidfVectorizer(max_features=TFIDF_MAX_FEATURES, ngram_range=TFIDF_NGRAM_RANGE)),
        ('clf', MultinomialNB())
    ])
    
    # Train
    print("\nâ³ Training...")
    model.fit(X_train, y_train)
    
    # Evaluate
    train_acc = model.score(X_train, y_train)
    test_acc = model.score(X_test, y_test)
    
    print(f"\nðŸ“ˆ Training Accuracy: {train_acc:.1%}")
    print(f"ðŸ“ˆ Test Accuracy: {test_acc:.1%}")
    
    # Cross-validation
    cv_scores = cross_val_score(model, X, y, cv=CV_FOLDS)
    print(f"ðŸ“Š Cross-val Score: {cv_scores.mean():.1%} (+/- {cv_scores.std()*2:.1%})")
    
    return model

model = train_model()

# %% [markdown]
# ## Step 6: Predict Uncategorized ðŸŽ¯

# %%
predictions_df = None

def predict_uncategorized():
    """Predict categories for uncategorized items! ðŸŽ¯"""
    global predictions_df
    
    print("\n" + "=" * 60)
    print("ðŸŽ¯ PREDICTING CATEGORIES")
    print("=" * 60)
    
    if model is None:
        print("âŒ Train model first!")
        return None
    
    if df_predict is None or len(df_predict) == 0:
        print("âœ… All items already categorized!")
        return None
    
    X_pred = df_predict['combined_text'].astype(str)
    
    # Predict with probabilities
    predictions = model.predict(X_pred)
    probabilities = model.predict_proba(X_pred).max(axis=1)
    
    predictions_df = df_predict.copy()
    predictions_df['predicted_category'] = predictions
    predictions_df['confidence'] = probabilities
    
    # Summary
    high_conf = (probabilities >= 0.7).sum()
    low_conf = (probabilities < 0.5).sum()
    
    print(f"\nðŸ“Š Predictions made: {len(predictions_df)}")
    print(f"âœ… High confidence (â‰¥70%): {high_conf}")
    print(f"âš ï¸ Low confidence (<50%): {low_conf}")
    
    print("\nðŸ“‹ Sample Predictions:")
    print("-" * 60)
    for _, row in predictions_df.head(5).iterrows():
        snippet_name = str(row.get('Snippet Name', '') or '')[:30]
        print(f"  '{snippet_name}' â†’ {row['predicted_category']} ({row['confidence']:.0%})")
    
    return predictions_df

predictions_df = predict_uncategorized()

# %% [markdown]
# ## Step 7: Review Low Confidence âš ï¸

# %%
def review_low_confidence(threshold=0.5):
    """Review low confidence predictions! âš ï¸"""
    if predictions_df is None:
        print("âŒ No predictions yet!")
        return
    
    low_conf = predictions_df[predictions_df['confidence'] < threshold]
    
    print(f"\nâš ï¸ {len(low_conf)} items need manual review:")
    print("-" * 60)
    
    for _, row in low_conf.head(10).iterrows():
        snippet_name = str(row.get('Snippet Name', '') or '')[:25]
        print(f"  '{snippet_name}' â†’ {row['predicted_category']} ({row['confidence']:.0%})")

review_low_confidence()

# %% [markdown]
# ## Step 8: Export Predictions ðŸ“¤

# %%
def export_predictions():
    """Export predictions to CSV! ðŸ“¤"""
    if predictions_df is None:
        print("âŒ No predictions to export!")
        return
    
    output_file = os.path.join(OUTPUT_FOLDER, "ml_predictions.csv")
    
    export_cols = ['Snippet Name', 'Content', 'predicted_category', 'confidence']
    available_cols = [c for c in export_cols if c in predictions_df.columns]
    
    predictions_df[available_cols].to_csv(output_file, index=False)
    print(f"âœ… Exported to: {output_file}")
    
    if IN_COLAB:
        from google.colab import files
        files.download(output_file)

export_predictions()

# %% [markdown]
# ## ðŸŽ¯ Quick Menu

# %%
def show_menu():
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘         ðŸ§  ML CATEGORIZER                             â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  prepare_training_data()      - Prepare data          â•‘
â•‘  train_model()                - Train ML model        â•‘
â•‘  predict_uncategorized()      - Make predictions      â•‘
â•‘  review_low_confidence(0.5)   - Review uncertain      â•‘
â•‘  export_predictions()         - Export to CSV         â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)

show_menu()

# %%
if __name__ == "__main__":
    print("\nðŸŽ‰ ML Categorizer ready!")
